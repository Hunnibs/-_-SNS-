{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hunnibs/-_-SNS-/blob/main/KoBART_summarization(789).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "5gfpZsaXfLnO",
        "outputId": "6164d56c-c470-4895-d078-dc5446dc0f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting awscli\n",
            "  Downloading awscli-1.24.6-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 8.7 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting botocore==1.26.6\n",
            "  Downloading botocore-1.26.6-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 59.8 MB/s \n",
            "\u001b[?25hCollecting PyYAML<5.5,>=3.10\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.4 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 83.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting python-dateutil<3.0.0,>=2.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 80.1 MB/s \n",
            "\u001b[?25hCollecting pyasn1>=0.1.3\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: six, urllib3, python-dateutil, jmespath, pyasn1, botocore, s3transfer, rsa, PyYAML, docutils, colorama, awscli\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-5.4.1 awscli-1.24.6 botocore-1.26.6 colorama-0.4.4 docutils-0.17.1 jmespath-1.0.0 pyasn1-0.4.8 python-dateutil-2.8.2 rsa-4.8 s3transfer-0.5.2 six-1.16.0 urllib3-1.26.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install awscli --ignore-installed six"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kmKiEmonzNq",
        "outputId": "e18b889d-f5c0-4343-d6a9-f8fe7eb2c32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4rXwTUk1gaM",
        "outputId": "885c1ccd-e511-4b69-9ab0-25ccc59512c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wn1QX7wfSGC",
        "outputId": "8cbf82e1-70aa-4166-b9f6-f70c41f56323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.26.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 17.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.5.18.1)\n",
            "Installing collected packages: urllib3, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "awscli 1.24.6 requires docutils<0.17,>=0.10, but you have docutils 0.17.1 which is incompatible.\n",
            "awscli 1.24.6 requires rsa<4.8,>=3.1.2, but you have rsa 4.8 which is incompatible.\u001b[0m\n",
            "Successfully installed torchdata-0.3.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loST6SY_fSIX",
        "outputId": "3b224f6d-b9cc-4527-ef86-c466dd37ba06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kobart\n",
            "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-vgn0b9su/kobart_73b5a49709884ad38055d41510d8c78d\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoBART /tmp/pip-install-vgn0b9su/kobart_73b5a49709884ad38055d41510d8c78d\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.23.6-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from kobart) (1.3.5)\n",
            "Collecting pytorch-lightning==1.2.1\n",
            "  Downloading pytorch_lightning-1.2.1-py3-none-any.whl (814 kB)\n",
            "\u001b[K     |████████████████████████████████| 814 kB 64.3 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n",
            "\u001b[?25hCollecting transformers==4.3.3\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.1->kobart) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.1->kobart) (1.21.6)\n",
            "Collecting fsspec[http]>=0.8.1\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 73.3 MB/s \n",
            "\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.1->kobart) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->kobart) (4.2.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (3.7.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.11.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.46.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3->kobart) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 93.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 81.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->kobart) (0.5.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->kobart) (1.0.0)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.6 in /usr/local/lib/python3.7/dist-packages (from boto3->kobart) (1.26.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.6->boto3->kobart) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3->kobart) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->kobart) (2022.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.1.0)\n",
            "Building wheels for collected packages: kobart, future, sacremoses\n",
            "  Building wheel for kobart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobart: filename=kobart-0.5.1-py3-none-any.whl size=9562 sha256=a30987b1f84794d29ba22ccd7d2c40fde3f3c70a5c660d2f50efc305057e3614\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hml9qagk/wheels/6e/55/c4/bd4fede223bc304089ac8da2a2099a69db3fcd4b0e853383f5\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0ba00de6a492ac366e665404dcd886cc21851d3ba84e392ecf8b413935d8fdf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d37534f8abb453f3d1b417276bdefa4581b4b2267486ead97345b948b6e063e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built kobart future sacremoses\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torch, tokenizers, sacremoses, PyYAML, future, transformers, pytorch-lightning, boto3, kobart\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 5.4.1\n",
            "    Uninstalling PyYAML-5.4.1:\n",
            "      Successfully uninstalled PyYAML-5.4.1\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchdata 0.3.0 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "awscli 1.24.6 requires docutils<0.17,>=0.10, but you have docutils 0.17.1 which is incompatible.\n",
            "awscli 1.24.6 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
            "awscli 1.24.6 requires rsa<4.8,>=3.1.2, but you have rsa 4.8 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 boto3-1.23.6 frozenlist-1.3.0 fsspec-2022.5.0 future-0.18.2 kobart-0.5.1 multidict-6.0.2 pytorch-lightning-1.2.1 sacremoses-0.0.53 tokenizers-0.10.3 torch-1.7.1 transformers-4.3.3 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "dkz0Uf00JCxZ",
        "outputId": "8b761b4f-63d2-464d-fa20-90905cd9afa7"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d79c0b32fb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/kobart_summarization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/kobart_summarization'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/kobart_summarization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj2zXt0hfSK4",
        "outputId": "fb3e7a15-59dd-450b-8325-0eeb339d1149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kobart_summarization'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 47 (delta 15), reused 39 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SeHeon-Park/kobart_summarization.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EwGpAtA9fSNK",
        "outputId": "2fdfa2ac-af0d-47aa-a841-f5af7bc1e484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kobart_summarization\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.7.1)\n",
            "Collecting pytorch-lightning==1.2.2\n",
            "  Downloading pytorch_lightning-1.2.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting transformers==4.8.2\n",
            "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 12.3 MB/s \n",
            "\u001b[?25hCollecting streamlit==1.1.0\n",
            "  Downloading streamlit-1.1.0-py2.py3-none-any.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 24.0 MB/s \n",
            "\u001b[?25hCollecting pyngrok==5.1.0\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 89.5 MB/s \n",
            "\u001b[?25hCollecting pydeck\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 34.4 MB/s \n",
            "\u001b[?25hCollecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.13.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 105.3 MB/s \n",
            "\u001b[?25hCollecting cuda-python==11.6.1\n",
            "  Downloading cuda_python-11.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.18.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (2.8.0)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (2022.5.0)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (4.64.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 4)) (3.7.0)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (4.2.4)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 76.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (21.4.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (2.8.2)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 85.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (6.0.1)\n",
            "Collecting validators\n",
            "  Downloading validators-0.19.0.tar.gz (30 kB)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.8-py3-none-manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (5.1.1)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (4.2.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from cuda-python==11.6.1->-r requirements.txt (line 9)) (0.29.30)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck->-r requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck->-r requirements.txt (line 7)) (7.7.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck->-r requirements.txt (line 7)) (5.1.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.1.3)\n",
            "Collecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-7.3.1-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 101.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->-r requirements.txt (line 8)) (1.0.0)\n",
            "Collecting tornado>=5.0\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->-r requirements.txt (line 8)) (5.4.8)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->-r requirements.txt (line 8)) (1.5.5)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.33.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.11.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (3.8.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (4.4.2)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (4.8.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (5.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck->-r requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (3.8.0)\n",
            "Requirement already satisfied: pyzmq>=22.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->-r requirements.txt (line 8)) (23.0.0)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->-r requirements.txt (line 8)) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (2.15.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->-r requirements.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==1.1.0->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.46.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (3.3.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 4)) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 4)) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (0.13.3)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 3)) (1.7.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (5.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2->-r requirements.txt (line 4)) (1.1.0)\n",
            "Building wheels for collected packages: pyngrok, blinker, validators\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=ee115670e649ea55dc6c6384c4e1a3f3518b66676ede40d45f9b4b82c6d512c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=2399197cc9ff819d8371dc3e95d9558e2734232a05741c03d95bf9b1c97bd3d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.19.0-py3-none-any.whl size=19553 sha256=ea0e489bc5134928b8e88d594a7c0074a4e73025c58ca25c870517d99bbeb4c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/5d/69/ff53a908b9f14fb7730a58fdede0fac4cdc99ef3624ec76d05\n",
            "Successfully built pyngrok blinker validators\n",
            "Installing collected packages: tornado, prompt-toolkit, jupyter-client, ipython, ipykernel, smmap, gitdb, watchdog, validators, toml, pydeck, huggingface-hub, gitpython, blinker, base58, transformers, streamlit, pytorch-lightning, pyngrok, cuda-python\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.3.3\n",
            "    Uninstalling transformers-4.3.3:\n",
            "      Successfully uninstalled transformers-4.3.3\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 1.2.1\n",
            "    Uninstalling pytorch-lightning-1.2.1:\n",
            "      Successfully uninstalled pytorch-lightning-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kobart 0.5.1 requires pytorch-lightning==1.2.1, but you have pytorch-lightning 1.2.2 which is incompatible.\n",
            "kobart 0.5.1 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed base58-2.1.1 blinker-1.4 cuda-python-11.6.1 gitdb-4.0.9 gitpython-3.1.27 huggingface-hub-0.0.12 ipykernel-6.13.0 ipython-7.33.0 jupyter-client-7.3.1 prompt-toolkit-3.0.29 pydeck-0.7.1 pyngrok-5.1.0 pytorch-lightning-1.2.2 smmap-5.0.0 streamlit-1.1.0 toml-0.10.2 tornado-6.1 transformers-4.8.2 validators-0.19.0 watchdog-2.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "jupyter_client",
                  "prompt_toolkit",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/kobart_summarization\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSZGqmqWfZKG",
        "outputId": "af79c6f7-e9f7-4bca-bb34-9036a143acc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/rocm4.5.2/torch-1.11.0%2Brocm4.5.2-cp37-cp37m-linux_x86_64.whl (1412.2 MB)\n",
            "\u001b[K     |███████████████████             | 834.1 MB 54.0 MB/s eta 0:00:11tcmalloc: large alloc 1147494400 bytes == 0x2150000 @  0x7f2ec2325615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████        | 1055.7 MB 1.2 MB/s eta 0:05:04tcmalloc: large alloc 1434370048 bytes == 0x467a6000 @  0x7f2ec2325615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |██████████████████████████████▎ | 1336.2 MB 68.0 MB/s eta 0:00:02tcmalloc: large alloc 1792966656 bytes == 0x9bf92000 @  0x7f2ec2325615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1412.2 MB 96.6 MB/s eta 0:00:01tcmalloc: large alloc 1765236736 bytes == 0x106d7a000 @  0x7f2ec2325615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1412.2 MB 8.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kobart 0.5.1 requires pytorch-lightning==1.2.1, but you have pytorch-lightning 1.2.2 which is incompatible.\n",
            "kobart 0.5.1 requires torch==1.7.1, but you have torch 1.11.0+rocm4.5.2 which is incompatible.\n",
            "kobart 0.5.1 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0+rocm4.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abp3igJZfSPp",
        "outputId": "ca4a54bb-9553-4bc5-def6-6b717d0fc43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Installing collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kobart 0.5.1 requires pytorch-lightning==1.2.1, but you have pytorch-lightning 1.2.2 which is incompatible.\n",
            "kobart 0.5.1 requires torch==1.7.1, but you have torch 1.11.0+rocm4.5.2 which is incompatible.\n",
            "kobart 0.5.1 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed requests-2.27.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdgv2y-IfhxL",
        "outputId": "50484795-cd42-4b1f-b9af-074c766e9d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=9, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='/content/drive/MyDrive/Colab_Notebooks/NLP/BART', default_root_dir='/content/drive/MyDrive/Colab_Notebooks/NLP/BART', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=1.0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=1, max_len=512, max_steps=None, min_epochs=None, min_steps=None, model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=1, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='/content/train.tsv', tpu_cores=<function _gpus_arg_default at 0x7f34dd0c7950>, track_grad_norm=-1, train_file='/content/test.tsv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "/content/kobart_summarization/.cache/kobart_base_cased_ff4bda5738.zip[██████████████████████████████████████████████████]\n",
            "/content/kobart_summarization/.cache/kobart_base_tokenizer_cased_cf74400bce.zip[██████████████████████████████████████████████████]\n",
            "using cached model. /content/kobart_summarization/.cache/kobart_base_tokenizer_cased_cf74400bce.zip\n",
            "GPU available: True, used: True\n",
            "INFO:lightning:GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
            "tcmalloc: large alloc 1073741824 bytes == 0x70ee8000 @  0x7f35477b02a4 0x7f3530bd99a5 0x7f3530bdacc1 0x7f3530bdc69e 0x7f3530bad50c 0x7f3530bba399 0x7f3530ba297a 0x59afff 0x515655 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x549576 0x4bca8a 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x532b86 0x594a96 0x515600 0x4bc98a 0x5134a6\n",
            "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:root:number of workers 1, data length 30122\n",
            "INFO:root:num_train_steps : 3346\n",
            "INFO:root:num_warmup_steps : 334\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "INFO:lightning:\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Epoch 0:  11% 3360/30456 [40:53<5:29:43,  1.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/27109 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  11% 3380/30456 [40:58<5:28:10,  1.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  11% 3400/30456 [41:02<5:26:38,  1.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  11% 3420/30456 [41:07<5:25:07,  1.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  11% 3440/30456 [41:12<5:23:37,  1.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  11% 3460/30456 [41:17<5:22:08,  1.40it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  11% 3480/30456 [41:22<5:20:40,  1.40it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  11% 3500/30456 [41:26<5:19:13,  1.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3520/30456 [41:31<5:17:47,  1.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3540/30456 [41:36<5:16:22,  1.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3560/30456 [41:41<5:14:58,  1.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3580/30456 [41:46<5:13:34,  1.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3600/30456 [41:51<5:12:12,  1.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3620/30456 [41:55<5:10:50,  1.44it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3640/30456 [42:00<5:09:29,  1.44it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3660/30456 [42:05<5:08:09,  1.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3680/30456 [42:10<5:06:50,  1.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3700/30456 [42:15<5:05:32,  1.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3720/30456 [42:19<5:04:14,  1.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3740/30456 [42:24<5:02:58,  1.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3760/30456 [42:29<5:01:42,  1.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3780/30456 [42:34<5:00:26,  1.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  12% 3800/30456 [42:39<4:59:12,  1.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3820/30456 [42:44<4:57:58,  1.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3840/30456 [42:48<4:56:45,  1.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3860/30456 [42:53<4:55:32,  1.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3880/30456 [42:58<4:54:21,  1.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3900/30456 [43:03<4:53:10,  1.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3920/30456 [43:08<4:52:00,  1.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3940/30456 [43:12<4:50:50,  1.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3960/30456 [43:17<4:49:41,  1.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 3980/30456 [43:22<4:48:33,  1.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 4000/30456 [43:27<4:47:25,  1.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 4020/30456 [43:32<4:46:18,  1.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 4040/30456 [43:37<4:45:11,  1.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 4060/30456 [43:41<4:44:05,  1.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 4080/30456 [43:46<4:43:00,  1.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  13% 4100/30456 [43:51<4:41:55,  1.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4120/30456 [43:56<4:40:51,  1.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4140/30456 [44:01<4:39:48,  1.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4160/30456 [44:05<4:38:45,  1.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4180/30456 [44:10<4:37:42,  1.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4200/30456 [44:15<4:36:41,  1.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4220/30456 [44:20<4:35:39,  1.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4240/30456 [44:25<4:34:39,  1.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4260/30456 [44:30<4:33:38,  1.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4280/30456 [44:34<4:32:39,  1.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4300/30456 [44:39<4:31:39,  1.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4320/30456 [44:44<4:30:41,  1.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4340/30456 [44:49<4:29:42,  1.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4360/30456 [44:54<4:28:45,  1.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4380/30456 [44:58<4:27:47,  1.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  14% 4400/30456 [45:03<4:26:51,  1.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4420/30456 [45:08<4:25:54,  1.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4440/30456 [45:13<4:24:58,  1.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4460/30456 [45:18<4:24:03,  1.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4480/30456 [45:23<4:23:08,  1.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4500/30456 [45:27<4:22:14,  1.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4520/30456 [45:32<4:21:20,  1.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4540/30456 [45:37<4:20:26,  1.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4560/30456 [45:42<4:19:33,  1.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4580/30456 [45:47<4:18:40,  1.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4600/30456 [45:51<4:17:48,  1.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4620/30456 [45:56<4:16:56,  1.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4640/30456 [46:01<4:16:04,  1.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4660/30456 [46:06<4:15:13,  1.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4680/30456 [46:11<4:14:22,  1.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4700/30456 [46:16<4:13:32,  1.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  15% 4720/30456 [46:20<4:12:42,  1.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4740/30456 [46:25<4:11:52,  1.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4760/30456 [46:30<4:11:03,  1.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4780/30456 [46:35<4:10:14,  1.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4800/30456 [46:40<4:09:26,  1.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4820/30456 [46:44<4:08:38,  1.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4840/30456 [46:49<4:07:50,  1.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4860/30456 [46:54<4:07:03,  1.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4880/30456 [46:59<4:06:16,  1.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4900/30456 [47:04<4:05:29,  1.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4920/30456 [47:08<4:04:43,  1.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4940/30456 [47:13<4:03:57,  1.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4960/30456 [47:18<4:03:11,  1.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 4980/30456 [47:23<4:02:26,  1.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 5000/30456 [47:28<4:01:41,  1.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  16% 5020/30456 [47:33<4:00:56,  1.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5040/30456 [47:37<4:00:11,  1.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5060/30456 [47:42<3:59:27,  1.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5080/30456 [47:47<3:58:44,  1.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5100/30456 [47:52<3:58:00,  1.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5120/30456 [47:57<3:57:17,  1.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5140/30456 [48:01<3:56:34,  1.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5160/30456 [48:06<3:55:52,  1.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5180/30456 [48:11<3:55:09,  1.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5200/30456 [48:16<3:54:27,  1.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5220/30456 [48:21<3:53:46,  1.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5240/30456 [48:26<3:53:04,  1.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5260/30456 [48:30<3:52:23,  1.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5280/30456 [48:35<3:51:42,  1.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5300/30456 [48:40<3:51:02,  1.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  17% 5320/30456 [48:45<3:50:21,  1.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5340/30456 [48:50<3:49:41,  1.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5360/30456 [48:54<3:49:01,  1.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5380/30456 [48:59<3:48:22,  1.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5400/30456 [49:04<3:47:43,  1.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5420/30456 [49:09<3:47:03,  1.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5440/30456 [49:14<3:46:25,  1.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5460/30456 [49:19<3:45:46,  1.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5480/30456 [49:23<3:45:08,  1.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5500/30456 [49:28<3:44:30,  1.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5520/30456 [49:33<3:43:52,  1.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5540/30456 [49:38<3:43:14,  1.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5560/30456 [49:43<3:42:37,  1.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5580/30456 [49:47<3:42:00,  1.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5600/30456 [49:52<3:41:23,  1.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  18% 5620/30456 [49:57<3:40:47,  1.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5640/30456 [50:02<3:40:10,  1.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5660/30456 [50:07<3:39:34,  1.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5680/30456 [50:12<3:38:58,  1.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5700/30456 [50:16<3:38:22,  1.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5720/30456 [50:21<3:37:47,  1.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5740/30456 [50:26<3:37:11,  1.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5760/30456 [50:31<3:36:36,  1.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5780/30456 [50:36<3:36:01,  1.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5800/30456 [50:40<3:35:27,  1.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5820/30456 [50:45<3:34:52,  1.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5840/30456 [50:50<3:34:18,  1.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5860/30456 [50:55<3:33:44,  1.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5880/30456 [51:00<3:33:10,  1.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5900/30456 [51:05<3:32:36,  1.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  19% 5920/30456 [51:09<3:32:03,  1.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 5940/30456 [51:14<3:31:30,  1.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 5960/30456 [51:19<3:30:56,  1.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 5980/30456 [51:24<3:30:24,  1.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6000/30456 [51:29<3:29:51,  1.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6020/30456 [51:33<3:29:18,  1.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6040/30456 [51:38<3:28:46,  1.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6060/30456 [51:43<3:28:14,  1.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6080/30456 [51:48<3:27:42,  1.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6100/30456 [51:53<3:27:10,  1.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6120/30456 [51:58<3:26:38,  1.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6140/30456 [52:02<3:26:07,  1.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6160/30456 [52:07<3:25:36,  1.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6180/30456 [52:12<3:25:04,  1.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6200/30456 [52:17<3:24:33,  1.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6220/30456 [52:22<3:24:03,  1.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  20% 6240/30456 [52:26<3:23:32,  1.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6260/30456 [52:31<3:23:02,  1.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6280/30456 [52:36<3:22:31,  1.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6300/30456 [52:41<3:22:01,  1.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6320/30456 [52:46<3:21:31,  2.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6340/30456 [52:51<3:21:01,  2.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6360/30456 [52:55<3:20:32,  2.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6380/30456 [53:00<3:20:02,  2.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6400/30456 [53:05<3:19:33,  2.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6420/30456 [53:10<3:19:04,  2.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6440/30456 [53:15<3:18:35,  2.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6460/30456 [53:19<3:18:06,  2.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6480/30456 [53:24<3:17:37,  2.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6500/30456 [53:29<3:17:09,  2.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6520/30456 [53:34<3:16:40,  2.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  21% 6540/30456 [53:39<3:16:12,  2.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6560/30456 [53:44<3:15:44,  2.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6580/30456 [53:48<3:15:16,  2.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6600/30456 [53:53<3:14:48,  2.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6620/30456 [53:58<3:14:20,  2.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6640/30456 [54:03<3:13:52,  2.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6660/30456 [54:08<3:13:25,  2.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6680/30456 [54:12<3:12:58,  2.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6700/30456 [54:17<3:12:30,  2.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6720/30456 [54:22<3:12:03,  2.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6740/30456 [54:27<3:11:36,  2.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6760/30456 [54:32<3:11:10,  2.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6780/30456 [54:37<3:10:43,  2.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6800/30456 [54:41<3:10:16,  2.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6820/30456 [54:46<3:09:50,  2.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  22% 6840/30456 [54:51<3:09:24,  2.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6860/30456 [54:56<3:08:58,  2.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6880/30456 [55:01<3:08:32,  2.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6900/30456 [55:05<3:08:06,  2.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6920/30456 [55:10<3:07:40,  2.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6940/30456 [55:15<3:07:14,  2.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6960/30456 [55:20<3:06:49,  2.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 6980/30456 [55:25<3:06:23,  2.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7000/30456 [55:30<3:05:58,  2.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7020/30456 [55:34<3:05:33,  2.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7040/30456 [55:39<3:05:08,  2.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7060/30456 [55:44<3:04:43,  2.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7080/30456 [55:49<3:04:18,  2.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7100/30456 [55:54<3:03:53,  2.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7120/30456 [55:58<3:03:28,  2.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  23% 7140/30456 [56:03<3:03:04,  2.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7160/30456 [56:08<3:02:40,  2.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7180/30456 [56:13<3:02:15,  2.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7200/30456 [56:18<3:01:51,  2.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7220/30456 [56:23<3:01:27,  2.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7240/30456 [56:27<3:01:03,  2.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7260/30456 [56:32<3:00:39,  2.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7280/30456 [56:37<3:00:15,  2.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7300/30456 [56:42<2:59:52,  2.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7320/30456 [56:47<2:59:28,  2.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7340/30456 [56:51<2:59:05,  2.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7360/30456 [56:56<2:58:41,  2.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7380/30456 [57:01<2:58:18,  2.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7400/30456 [57:06<2:57:55,  2.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7420/30456 [57:11<2:57:32,  2.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7440/30456 [57:16<2:57:09,  2.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  24% 7460/30456 [57:20<2:56:46,  2.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7480/30456 [57:25<2:56:23,  2.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7500/30456 [57:30<2:56:01,  2.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7520/30456 [57:35<2:55:38,  2.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7540/30456 [57:40<2:55:16,  2.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7560/30456 [57:44<2:54:53,  2.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7580/30456 [57:49<2:54:31,  2.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7600/30456 [57:54<2:54:09,  2.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7620/30456 [57:59<2:53:47,  2.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7640/30456 [58:04<2:53:25,  2.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7660/30456 [58:09<2:53:03,  2.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7680/30456 [58:13<2:52:41,  2.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7700/30456 [58:18<2:52:19,  2.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7720/30456 [58:23<2:51:57,  2.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7740/30456 [58:28<2:51:36,  2.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  25% 7760/30456 [58:33<2:51:14,  2.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7780/30456 [58:37<2:50:53,  2.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7800/30456 [58:42<2:50:32,  2.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7820/30456 [58:47<2:50:10,  2.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7840/30456 [58:52<2:49:49,  2.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7860/30456 [58:57<2:49:28,  2.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7880/30456 [59:01<2:49:07,  2.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7900/30456 [59:06<2:48:46,  2.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7920/30456 [59:11<2:48:26,  2.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7940/30456 [59:16<2:48:05,  2.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7960/30456 [59:21<2:47:44,  2.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 7980/30456 [59:26<2:47:24,  2.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 8000/30456 [59:30<2:47:03,  2.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 8020/30456 [59:35<2:46:43,  2.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 8040/30456 [59:40<2:46:22,  2.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  26% 8060/30456 [59:45<2:46:02,  2.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8080/30456 [59:50<2:45:42,  2.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8100/30456 [59:54<2:45:22,  2.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8120/30456 [59:59<2:45:02,  2.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8140/30456 [1:00:04<2:44:42,  2.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8160/30456 [1:00:09<2:44:22,  2.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8180/30456 [1:00:14<2:44:02,  2.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8200/30456 [1:00:19<2:43:42,  2.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8220/30456 [1:00:23<2:43:23,  2.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8240/30456 [1:00:28<2:43:03,  2.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8260/30456 [1:00:33<2:42:43,  2.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8280/30456 [1:00:38<2:42:24,  2.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8300/30456 [1:00:43<2:42:05,  2.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8320/30456 [1:00:47<2:41:45,  2.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8340/30456 [1:00:52<2:41:26,  2.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  27% 8360/30456 [1:00:57<2:41:07,  2.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8380/30456 [1:01:02<2:40:48,  2.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8400/30456 [1:01:07<2:40:29,  2.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8420/30456 [1:01:12<2:40:10,  2.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8440/30456 [1:01:16<2:39:51,  2.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8460/30456 [1:01:21<2:39:32,  2.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8480/30456 [1:01:26<2:39:13,  2.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8500/30456 [1:01:31<2:38:54,  2.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8520/30456 [1:01:36<2:38:36,  2.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8540/30456 [1:01:40<2:38:17,  2.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8560/30456 [1:01:45<2:37:59,  2.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8580/30456 [1:01:50<2:37:40,  2.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8600/30456 [1:01:55<2:37:22,  2.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8620/30456 [1:02:00<2:37:04,  2.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8640/30456 [1:02:05<2:36:45,  2.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  28% 8660/30456 [1:02:09<2:36:27,  2.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8680/30456 [1:02:14<2:36:09,  2.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8700/30456 [1:02:19<2:35:51,  2.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8720/30456 [1:02:24<2:35:33,  2.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8740/30456 [1:02:29<2:35:15,  2.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8760/30456 [1:02:33<2:34:57,  2.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8780/30456 [1:02:38<2:34:39,  2.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8800/30456 [1:02:43<2:34:21,  2.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8820/30456 [1:02:48<2:34:04,  2.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8840/30456 [1:02:53<2:33:46,  2.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8860/30456 [1:02:58<2:33:28,  2.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8880/30456 [1:03:02<2:33:11,  2.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8900/30456 [1:03:07<2:32:53,  2.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8920/30456 [1:03:12<2:32:36,  2.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8940/30456 [1:03:17<2:32:19,  2.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8960/30456 [1:03:22<2:32:01,  2.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  29% 8980/30456 [1:03:26<2:31:44,  2.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9000/30456 [1:03:31<2:31:27,  2.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9020/30456 [1:03:36<2:31:10,  2.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9040/30456 [1:03:41<2:30:53,  2.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9060/30456 [1:03:46<2:30:36,  2.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9080/30456 [1:03:51<2:30:19,  2.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9100/30456 [1:03:55<2:30:02,  2.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9120/30456 [1:04:00<2:29:45,  2.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9140/30456 [1:04:05<2:29:28,  2.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9160/30456 [1:04:10<2:29:11,  2.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9180/30456 [1:04:15<2:28:54,  2.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9200/30456 [1:04:19<2:28:38,  2.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9220/30456 [1:04:24<2:28:21,  2.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9240/30456 [1:04:29<2:28:05,  2.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9260/30456 [1:04:34<2:27:48,  2.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  30% 9280/30456 [1:04:39<2:27:32,  2.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9300/30456 [1:04:44<2:27:15,  2.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9320/30456 [1:04:48<2:26:59,  2.40it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9340/30456 [1:04:53<2:26:42,  2.40it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9360/30456 [1:04:58<2:26:26,  2.40it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9380/30456 [1:05:03<2:26:10,  2.40it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9400/30456 [1:05:08<2:25:54,  2.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9420/30456 [1:05:12<2:25:38,  2.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9440/30456 [1:05:17<2:25:22,  2.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9460/30456 [1:05:22<2:25:06,  2.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9480/30456 [1:05:27<2:24:50,  2.41it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9500/30456 [1:05:32<2:24:34,  2.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9520/30456 [1:05:37<2:24:18,  2.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9540/30456 [1:05:41<2:24:02,  2.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9560/30456 [1:05:46<2:23:46,  2.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  31% 9580/30456 [1:05:51<2:23:30,  2.42it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9600/30456 [1:05:56<2:23:15,  2.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9620/30456 [1:06:01<2:22:59,  2.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9640/30456 [1:06:05<2:22:43,  2.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9660/30456 [1:06:10<2:22:28,  2.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9680/30456 [1:06:15<2:22:12,  2.43it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9700/30456 [1:06:20<2:21:57,  2.44it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9720/30456 [1:06:25<2:21:41,  2.44it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9740/30456 [1:06:30<2:21:26,  2.44it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9760/30456 [1:06:34<2:21:11,  2.44it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9780/30456 [1:06:39<2:20:55,  2.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9800/30456 [1:06:44<2:20:40,  2.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9820/30456 [1:06:49<2:20:25,  2.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9840/30456 [1:06:54<2:20:10,  2.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9860/30456 [1:06:58<2:19:54,  2.45it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  32% 9880/30456 [1:07:03<2:19:39,  2.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 9900/30456 [1:07:08<2:19:24,  2.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 9920/30456 [1:07:13<2:19:09,  2.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 9940/30456 [1:07:18<2:18:54,  2.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 9960/30456 [1:07:23<2:18:39,  2.46it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 9980/30456 [1:07:27<2:18:25,  2.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10000/30456 [1:07:32<2:18:10,  2.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10020/30456 [1:07:37<2:17:55,  2.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10040/30456 [1:07:42<2:17:40,  2.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10060/30456 [1:07:47<2:17:25,  2.47it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10080/30456 [1:07:51<2:17:11,  2.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10100/30456 [1:07:56<2:16:56,  2.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10120/30456 [1:08:01<2:16:41,  2.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10140/30456 [1:08:06<2:16:27,  2.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10160/30456 [1:08:11<2:16:12,  2.48it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10180/30456 [1:08:16<2:15:58,  2.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  33% 10200/30456 [1:08:20<2:15:43,  2.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10220/30456 [1:08:25<2:15:29,  2.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10240/30456 [1:08:30<2:15:15,  2.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10260/30456 [1:08:35<2:15:00,  2.49it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10280/30456 [1:08:40<2:14:46,  2.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10300/30456 [1:08:44<2:14:32,  2.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10320/30456 [1:08:49<2:14:17,  2.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10340/30456 [1:08:54<2:14:03,  2.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10360/30456 [1:08:59<2:13:49,  2.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10380/30456 [1:09:04<2:13:35,  2.50it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10400/30456 [1:09:09<2:13:21,  2.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10420/30456 [1:09:13<2:13:07,  2.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10440/30456 [1:09:18<2:12:53,  2.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10460/30456 [1:09:23<2:12:39,  2.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10480/30456 [1:09:28<2:12:25,  2.51it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  34% 10500/30456 [1:09:33<2:12:11,  2.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10520/30456 [1:09:37<2:11:57,  2.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10540/30456 [1:09:42<2:11:43,  2.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10560/30456 [1:09:47<2:11:29,  2.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10580/30456 [1:09:52<2:11:15,  2.52it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10600/30456 [1:09:57<2:11:02,  2.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10620/30456 [1:10:02<2:10:48,  2.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10640/30456 [1:10:06<2:10:34,  2.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10660/30456 [1:10:11<2:10:21,  2.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10680/30456 [1:10:16<2:10:07,  2.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10700/30456 [1:10:21<2:09:53,  2.53it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10720/30456 [1:10:26<2:09:40,  2.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10740/30456 [1:10:30<2:09:26,  2.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10760/30456 [1:10:35<2:09:13,  2.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10780/30456 [1:10:40<2:09:00,  2.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  35% 10800/30456 [1:10:45<2:08:46,  2.54it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10820/30456 [1:10:50<2:08:33,  2.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10840/30456 [1:10:55<2:08:19,  2.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10860/30456 [1:10:59<2:08:06,  2.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10880/30456 [1:11:04<2:07:53,  2.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10900/30456 [1:11:09<2:07:39,  2.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10920/30456 [1:11:14<2:07:26,  2.55it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10940/30456 [1:11:19<2:07:13,  2.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10960/30456 [1:11:23<2:07:00,  2.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 10980/30456 [1:11:28<2:06:47,  2.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 11000/30456 [1:11:33<2:06:34,  2.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 11020/30456 [1:11:38<2:06:21,  2.56it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 11040/30456 [1:11:43<2:06:08,  2.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 11060/30456 [1:11:48<2:05:54,  2.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 11080/30456 [1:11:52<2:05:41,  2.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  36% 11100/30456 [1:11:57<2:05:29,  2.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11120/30456 [1:12:02<2:05:16,  2.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11140/30456 [1:12:07<2:05:03,  2.57it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11160/30456 [1:12:12<2:04:50,  2.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11180/30456 [1:12:16<2:04:37,  2.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11200/30456 [1:12:21<2:04:24,  2.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11220/30456 [1:12:26<2:04:11,  2.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11240/30456 [1:12:31<2:03:59,  2.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11260/30456 [1:12:36<2:03:46,  2.58it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11280/30456 [1:12:41<2:03:33,  2.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11300/30456 [1:12:45<2:03:21,  2.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11320/30456 [1:12:50<2:03:08,  2.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11340/30456 [1:12:55<2:02:55,  2.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11360/30456 [1:13:00<2:02:43,  2.59it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11380/30456 [1:13:05<2:02:30,  2.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11400/30456 [1:13:09<2:02:18,  2.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  37% 11420/30456 [1:13:14<2:02:05,  2.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11440/30456 [1:13:19<2:01:53,  2.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11460/30456 [1:13:24<2:01:40,  2.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11480/30456 [1:13:29<2:01:28,  2.60it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11500/30456 [1:13:34<2:01:15,  2.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11520/30456 [1:13:38<2:01:03,  2.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11540/30456 [1:13:43<2:00:51,  2.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11560/30456 [1:13:48<2:00:38,  2.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11580/30456 [1:13:53<2:00:26,  2.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11600/30456 [1:13:58<2:00:14,  2.61it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11620/30456 [1:14:02<2:00:01,  2.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11640/30456 [1:14:07<1:59:49,  2.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11660/30456 [1:14:12<1:59:37,  2.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11680/30456 [1:14:17<1:59:25,  2.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11700/30456 [1:14:22<1:59:13,  2.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  38% 11720/30456 [1:14:27<1:59:01,  2.62it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11740/30456 [1:14:31<1:58:49,  2.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11760/30456 [1:14:36<1:58:36,  2.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11780/30456 [1:14:41<1:58:24,  2.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11800/30456 [1:14:46<1:58:12,  2.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11820/30456 [1:14:51<1:58:00,  2.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11840/30456 [1:14:55<1:57:48,  2.63it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11860/30456 [1:15:00<1:57:36,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11880/30456 [1:15:05<1:57:25,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11900/30456 [1:15:10<1:57:13,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11920/30456 [1:15:15<1:57:01,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11940/30456 [1:15:19<1:56:49,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11960/30456 [1:15:24<1:56:37,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 11980/30456 [1:15:29<1:56:25,  2.64it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 12000/30456 [1:15:34<1:56:13,  2.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  39% 12020/30456 [1:15:39<1:56:02,  2.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12040/30456 [1:15:44<1:55:50,  2.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12060/30456 [1:15:48<1:55:38,  2.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12080/30456 [1:15:53<1:55:27,  2.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12100/30456 [1:15:58<1:55:15,  2.65it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12120/30456 [1:16:03<1:55:03,  2.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12140/30456 [1:16:08<1:54:52,  2.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12160/30456 [1:16:12<1:54:40,  2.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12180/30456 [1:16:17<1:54:28,  2.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12200/30456 [1:16:22<1:54:17,  2.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12220/30456 [1:16:27<1:54:05,  2.66it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12240/30456 [1:16:32<1:53:54,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12260/30456 [1:16:37<1:53:42,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12280/30456 [1:16:41<1:53:31,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12300/30456 [1:16:46<1:53:19,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  40% 12320/30456 [1:16:51<1:53:08,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12340/30456 [1:16:56<1:52:57,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12360/30456 [1:17:01<1:52:45,  2.67it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12380/30456 [1:17:05<1:52:34,  2.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12400/30456 [1:17:10<1:52:23,  2.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12420/30456 [1:17:15<1:52:11,  2.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12440/30456 [1:17:20<1:52:00,  2.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12460/30456 [1:17:25<1:51:49,  2.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12480/30456 [1:17:30<1:51:37,  2.68it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12500/30456 [1:17:34<1:51:26,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12520/30456 [1:17:39<1:51:15,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12540/30456 [1:17:44<1:51:04,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12560/30456 [1:17:49<1:50:53,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12580/30456 [1:17:54<1:50:41,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12600/30456 [1:17:58<1:50:30,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  41% 12620/30456 [1:18:03<1:50:19,  2.69it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12640/30456 [1:18:08<1:50:08,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12660/30456 [1:18:13<1:49:57,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12680/30456 [1:18:18<1:49:46,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12700/30456 [1:18:23<1:49:35,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12720/30456 [1:18:27<1:49:24,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12740/30456 [1:18:32<1:49:13,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12760/30456 [1:18:37<1:49:02,  2.70it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12780/30456 [1:18:42<1:48:51,  2.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12800/30456 [1:18:47<1:48:40,  2.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12820/30456 [1:18:51<1:48:29,  2.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12840/30456 [1:18:56<1:48:18,  2.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12860/30456 [1:19:01<1:48:07,  2.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12880/30456 [1:19:06<1:47:56,  2.71it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12900/30456 [1:19:11<1:47:46,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12920/30456 [1:19:16<1:47:35,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  42% 12940/30456 [1:19:20<1:47:24,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 12960/30456 [1:19:25<1:47:13,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 12980/30456 [1:19:30<1:47:02,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13000/30456 [1:19:35<1:46:52,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13020/30456 [1:19:40<1:46:41,  2.72it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13040/30456 [1:19:44<1:46:30,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13060/30456 [1:19:49<1:46:20,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13080/30456 [1:19:54<1:46:09,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13100/30456 [1:19:59<1:45:58,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13120/30456 [1:20:04<1:45:48,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13140/30456 [1:20:09<1:45:37,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13160/30456 [1:20:13<1:45:26,  2.73it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13180/30456 [1:20:18<1:45:16,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13200/30456 [1:20:23<1:45:05,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13220/30456 [1:20:28<1:44:55,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  43% 13240/30456 [1:20:33<1:44:44,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13260/30456 [1:20:37<1:44:34,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13280/30456 [1:20:42<1:44:23,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13300/30456 [1:20:47<1:44:13,  2.74it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13320/30456 [1:20:52<1:44:02,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13340/30456 [1:20:57<1:43:52,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13360/30456 [1:21:02<1:43:41,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13380/30456 [1:21:06<1:43:31,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13400/30456 [1:21:11<1:43:20,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13420/30456 [1:21:16<1:43:10,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13440/30456 [1:21:21<1:43:00,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13460/30456 [1:21:26<1:42:49,  2.75it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13480/30456 [1:21:30<1:42:39,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13500/30456 [1:21:35<1:42:29,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13520/30456 [1:21:40<1:42:18,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  44% 13540/30456 [1:21:45<1:42:08,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13560/30456 [1:21:50<1:41:58,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13580/30456 [1:21:55<1:41:47,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13600/30456 [1:21:59<1:41:37,  2.76it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13620/30456 [1:22:04<1:41:27,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13640/30456 [1:22:09<1:41:17,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13660/30456 [1:22:14<1:41:07,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13680/30456 [1:22:19<1:40:56,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13700/30456 [1:22:23<1:40:46,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13720/30456 [1:22:28<1:40:36,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13740/30456 [1:22:33<1:40:26,  2.77it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13760/30456 [1:22:38<1:40:16,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13780/30456 [1:22:43<1:40:06,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13800/30456 [1:22:48<1:39:56,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13820/30456 [1:22:52<1:39:46,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  45% 13840/30456 [1:22:57<1:39:36,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13860/30456 [1:23:02<1:39:26,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13880/30456 [1:23:07<1:39:16,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13900/30456 [1:23:12<1:39:06,  2.78it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13920/30456 [1:23:16<1:38:56,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13940/30456 [1:23:21<1:38:46,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13960/30456 [1:23:26<1:38:36,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 13980/30456 [1:23:31<1:38:26,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14000/30456 [1:23:36<1:38:16,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14020/30456 [1:23:41<1:38:06,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14040/30456 [1:23:45<1:37:56,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14060/30456 [1:23:50<1:37:46,  2.79it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14080/30456 [1:23:55<1:37:36,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14100/30456 [1:24:00<1:37:26,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14120/30456 [1:24:05<1:37:16,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14140/30456 [1:24:09<1:37:07,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  46% 14160/30456 [1:24:14<1:36:57,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14180/30456 [1:24:19<1:36:47,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14200/30456 [1:24:24<1:36:37,  2.80it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14220/30456 [1:24:29<1:36:27,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14240/30456 [1:24:34<1:36:18,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14260/30456 [1:24:38<1:36:08,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14280/30456 [1:24:43<1:35:58,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14300/30456 [1:24:48<1:35:48,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14320/30456 [1:24:53<1:35:39,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14340/30456 [1:24:58<1:35:29,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14360/30456 [1:25:02<1:35:19,  2.81it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14380/30456 [1:25:07<1:35:10,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14400/30456 [1:25:12<1:35:00,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14420/30456 [1:25:17<1:34:50,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14440/30456 [1:25:22<1:34:41,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  47% 14460/30456 [1:25:27<1:34:31,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14480/30456 [1:25:31<1:34:22,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14500/30456 [1:25:36<1:34:12,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14520/30456 [1:25:41<1:34:02,  2.82it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14540/30456 [1:25:46<1:33:53,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14560/30456 [1:25:51<1:33:43,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14580/30456 [1:25:55<1:33:34,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14600/30456 [1:26:00<1:33:24,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14620/30456 [1:26:05<1:33:15,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14640/30456 [1:26:10<1:33:05,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14660/30456 [1:26:15<1:32:56,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14680/30456 [1:26:19<1:32:46,  2.83it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14700/30456 [1:26:24<1:32:37,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14720/30456 [1:26:29<1:32:27,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14740/30456 [1:26:34<1:32:18,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  48% 14760/30456 [1:26:39<1:32:08,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14780/30456 [1:26:44<1:31:59,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14800/30456 [1:26:48<1:31:50,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14820/30456 [1:26:53<1:31:40,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14840/30456 [1:26:58<1:31:31,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14860/30456 [1:27:03<1:31:22,  2.84it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14880/30456 [1:27:08<1:31:12,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14900/30456 [1:27:12<1:31:03,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14920/30456 [1:27:17<1:30:54,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14940/30456 [1:27:22<1:30:44,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14960/30456 [1:27:27<1:30:35,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 14980/30456 [1:27:32<1:30:26,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 15000/30456 [1:27:37<1:30:16,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 15020/30456 [1:27:41<1:30:07,  2.85it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 15040/30456 [1:27:46<1:29:58,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  49% 15060/30456 [1:27:51<1:29:49,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15080/30456 [1:27:56<1:29:39,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15100/30456 [1:28:01<1:29:30,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15120/30456 [1:28:05<1:29:21,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15140/30456 [1:28:10<1:29:12,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15160/30456 [1:28:15<1:29:03,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15180/30456 [1:28:20<1:28:53,  2.86it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15200/30456 [1:28:25<1:28:44,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15220/30456 [1:28:30<1:28:35,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15240/30456 [1:28:34<1:28:26,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15260/30456 [1:28:39<1:28:17,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15280/30456 [1:28:44<1:28:08,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15300/30456 [1:28:49<1:27:59,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15320/30456 [1:28:54<1:27:50,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15340/30456 [1:28:58<1:27:41,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15360/30456 [1:29:03<1:27:31,  2.87it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  50% 15380/30456 [1:29:08<1:27:22,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15400/30456 [1:29:13<1:27:13,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15420/30456 [1:29:18<1:27:04,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15440/30456 [1:29:23<1:26:55,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15460/30456 [1:29:27<1:26:46,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15480/30456 [1:29:32<1:26:37,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15500/30456 [1:29:37<1:26:28,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15520/30456 [1:29:42<1:26:19,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15540/30456 [1:29:47<1:26:10,  2.88it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15560/30456 [1:29:51<1:26:01,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15580/30456 [1:29:56<1:25:52,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15600/30456 [1:30:01<1:25:43,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15620/30456 [1:30:06<1:25:35,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15640/30456 [1:30:11<1:25:26,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15660/30456 [1:30:16<1:25:17,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  51% 15680/30456 [1:30:20<1:25:08,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15700/30456 [1:30:25<1:24:59,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15720/30456 [1:30:30<1:24:50,  2.89it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15740/30456 [1:30:35<1:24:41,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15760/30456 [1:30:40<1:24:32,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15780/30456 [1:30:44<1:24:24,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15800/30456 [1:30:49<1:24:15,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15820/30456 [1:30:54<1:24:06,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15840/30456 [1:30:59<1:23:57,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15860/30456 [1:31:04<1:23:48,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15880/30456 [1:31:09<1:23:39,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15900/30456 [1:31:13<1:23:31,  2.90it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15920/30456 [1:31:18<1:23:22,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15940/30456 [1:31:23<1:23:13,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15960/30456 [1:31:28<1:23:04,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  52% 15980/30456 [1:31:33<1:22:56,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16000/30456 [1:31:37<1:22:47,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16020/30456 [1:31:42<1:22:38,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16040/30456 [1:31:47<1:22:29,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16060/30456 [1:31:52<1:22:21,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16080/30456 [1:31:57<1:22:12,  2.91it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16100/30456 [1:32:02<1:22:03,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16120/30456 [1:32:06<1:21:55,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16140/30456 [1:32:11<1:21:46,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16160/30456 [1:32:16<1:21:37,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16180/30456 [1:32:21<1:21:29,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16200/30456 [1:32:26<1:21:20,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16220/30456 [1:32:30<1:21:11,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16240/30456 [1:32:35<1:21:03,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16260/30456 [1:32:40<1:20:54,  2.92it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  53% 16280/30456 [1:32:45<1:20:46,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16300/30456 [1:32:50<1:20:37,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16320/30456 [1:32:55<1:20:28,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16340/30456 [1:32:59<1:20:20,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16360/30456 [1:33:04<1:20:11,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16380/30456 [1:33:09<1:20:03,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16400/30456 [1:33:14<1:19:54,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16420/30456 [1:33:19<1:19:46,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16440/30456 [1:33:23<1:19:37,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16460/30456 [1:33:28<1:19:29,  2.93it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16480/30456 [1:33:33<1:19:20,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16500/30456 [1:33:38<1:19:12,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16520/30456 [1:33:43<1:19:03,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16540/30456 [1:33:48<1:18:55,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16560/30456 [1:33:52<1:18:46,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  54% 16580/30456 [1:33:57<1:18:38,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16600/30456 [1:34:02<1:18:29,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16620/30456 [1:34:07<1:18:21,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16640/30456 [1:34:12<1:18:12,  2.94it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16660/30456 [1:34:16<1:18:04,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16680/30456 [1:34:21<1:17:56,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16700/30456 [1:34:26<1:17:47,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16720/30456 [1:34:31<1:17:39,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16740/30456 [1:34:36<1:17:30,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16760/30456 [1:34:41<1:17:22,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16780/30456 [1:34:45<1:17:14,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16800/30456 [1:34:50<1:17:05,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16820/30456 [1:34:55<1:16:57,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16840/30456 [1:35:00<1:16:48,  2.95it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16860/30456 [1:35:05<1:16:40,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16880/30456 [1:35:09<1:16:32,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  55% 16900/30456 [1:35:14<1:16:23,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 16920/30456 [1:35:19<1:16:15,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 16940/30456 [1:35:24<1:16:07,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 16960/30456 [1:35:29<1:15:59,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 16980/30456 [1:35:33<1:15:50,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17000/30456 [1:35:38<1:15:42,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17020/30456 [1:35:43<1:15:34,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17040/30456 [1:35:48<1:15:25,  2.96it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17060/30456 [1:35:53<1:15:17,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17080/30456 [1:35:58<1:15:09,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17100/30456 [1:36:02<1:15:01,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17120/30456 [1:36:07<1:14:52,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17140/30456 [1:36:12<1:14:44,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17160/30456 [1:36:17<1:14:36,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17180/30456 [1:36:22<1:14:28,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  56% 17200/30456 [1:36:26<1:14:20,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17220/30456 [1:36:31<1:14:11,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17240/30456 [1:36:36<1:14:03,  2.97it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17260/30456 [1:36:41<1:13:55,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17280/30456 [1:36:46<1:13:47,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17300/30456 [1:36:51<1:13:39,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17320/30456 [1:36:55<1:13:30,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17340/30456 [1:37:00<1:13:22,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17360/30456 [1:37:05<1:13:14,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17380/30456 [1:37:10<1:13:06,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17400/30456 [1:37:15<1:12:58,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17420/30456 [1:37:19<1:12:50,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17440/30456 [1:37:24<1:12:42,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17460/30456 [1:37:29<1:12:34,  2.98it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17480/30456 [1:37:34<1:12:25,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  57% 17500/30456 [1:37:39<1:12:17,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17520/30456 [1:37:44<1:12:09,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17540/30456 [1:37:48<1:12:01,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17560/30456 [1:37:53<1:11:53,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17580/30456 [1:37:58<1:11:45,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17600/30456 [1:38:03<1:11:37,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17620/30456 [1:38:08<1:11:29,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17640/30456 [1:38:12<1:11:21,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17660/30456 [1:38:17<1:11:13,  2.99it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17680/30456 [1:38:22<1:11:05,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17700/30456 [1:38:27<1:10:57,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17720/30456 [1:38:32<1:10:49,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17740/30456 [1:38:37<1:10:41,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17760/30456 [1:38:41<1:10:33,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17780/30456 [1:38:46<1:10:25,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  58% 17800/30456 [1:38:51<1:10:17,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17820/30456 [1:38:56<1:10:09,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17840/30456 [1:39:01<1:10:01,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17860/30456 [1:39:05<1:09:53,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17880/30456 [1:39:10<1:09:45,  3.00it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17900/30456 [1:39:15<1:09:37,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17920/30456 [1:39:20<1:09:29,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17940/30456 [1:39:25<1:09:21,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17960/30456 [1:39:30<1:09:13,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 17980/30456 [1:39:34<1:09:05,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18000/30456 [1:39:39<1:08:57,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18020/30456 [1:39:44<1:08:50,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18040/30456 [1:39:49<1:08:42,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18060/30456 [1:39:54<1:08:34,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18080/30456 [1:39:58<1:08:26,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18100/30456 [1:40:03<1:08:18,  3.01it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  59% 18120/30456 [1:40:08<1:08:10,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18140/30456 [1:40:13<1:08:02,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18160/30456 [1:40:18<1:07:54,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18180/30456 [1:40:23<1:07:47,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18200/30456 [1:40:27<1:07:39,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18220/30456 [1:40:32<1:07:31,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18240/30456 [1:40:37<1:07:23,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18260/30456 [1:40:42<1:07:15,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18280/30456 [1:40:47<1:07:07,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18300/30456 [1:40:51<1:07:00,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18320/30456 [1:40:56<1:06:52,  3.02it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18340/30456 [1:41:01<1:06:44,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18360/30456 [1:41:06<1:06:36,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18380/30456 [1:41:11<1:06:28,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18400/30456 [1:41:16<1:06:21,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  60% 18420/30456 [1:41:20<1:06:13,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18440/30456 [1:41:25<1:06:05,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18460/30456 [1:41:30<1:05:57,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18480/30456 [1:41:35<1:05:50,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18500/30456 [1:41:40<1:05:42,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18520/30456 [1:41:44<1:05:34,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18540/30456 [1:41:49<1:05:26,  3.03it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18560/30456 [1:41:54<1:05:19,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18580/30456 [1:41:59<1:05:11,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18600/30456 [1:42:04<1:05:03,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18620/30456 [1:42:09<1:04:55,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18640/30456 [1:42:13<1:04:48,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18660/30456 [1:42:18<1:04:40,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18680/30456 [1:42:23<1:04:32,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18700/30456 [1:42:28<1:04:25,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  61% 18720/30456 [1:42:33<1:04:17,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18740/30456 [1:42:37<1:04:09,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18760/30456 [1:42:42<1:04:02,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18780/30456 [1:42:47<1:03:54,  3.04it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18800/30456 [1:42:52<1:03:46,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18820/30456 [1:42:57<1:03:39,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18840/30456 [1:43:02<1:03:31,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18860/30456 [1:43:06<1:03:23,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18880/30456 [1:43:11<1:03:16,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18900/30456 [1:43:16<1:03:08,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18920/30456 [1:43:21<1:03:01,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18940/30456 [1:43:26<1:02:53,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18960/30456 [1:43:30<1:02:45,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 18980/30456 [1:43:35<1:02:38,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 19000/30456 [1:43:40<1:02:30,  3.05it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  62% 19020/30456 [1:43:45<1:02:23,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19040/30456 [1:43:50<1:02:15,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19060/30456 [1:43:55<1:02:07,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19080/30456 [1:43:59<1:02:00,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19100/30456 [1:44:04<1:01:52,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19120/30456 [1:44:09<1:01:45,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19140/30456 [1:44:14<1:01:37,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19160/30456 [1:44:19<1:01:30,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19180/30456 [1:44:23<1:01:22,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19200/30456 [1:44:28<1:01:15,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19220/30456 [1:44:33<1:01:07,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19240/30456 [1:44:38<1:00:59,  3.06it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19260/30456 [1:44:43<1:00:52,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19280/30456 [1:44:48<1:00:44,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19300/30456 [1:44:52<1:00:37,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  63% 19320/30456 [1:44:57<1:00:29,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19340/30456 [1:45:02<1:00:22,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19360/30456 [1:45:07<1:00:14,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19380/30456 [1:45:12<1:00:07,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19400/30456 [1:45:16<59:59,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]  \n",
            "Epoch 0:  64% 19420/30456 [1:45:21<59:52,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19440/30456 [1:45:26<59:45,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19460/30456 [1:45:31<59:37,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19480/30456 [1:45:36<59:30,  3.07it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19500/30456 [1:45:41<59:22,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19520/30456 [1:45:45<59:15,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19540/30456 [1:45:50<59:07,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19560/30456 [1:45:55<59:00,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19580/30456 [1:46:00<58:52,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19600/30456 [1:46:05<58:45,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19620/30456 [1:46:09<58:38,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  64% 19640/30456 [1:46:14<58:30,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19660/30456 [1:46:19<58:23,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19680/30456 [1:46:24<58:15,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19700/30456 [1:46:29<58:08,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19720/30456 [1:46:34<58:01,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19740/30456 [1:46:38<57:53,  3.08it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19760/30456 [1:46:43<57:46,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19780/30456 [1:46:48<57:38,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19800/30456 [1:46:53<57:31,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19820/30456 [1:46:58<57:24,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19840/30456 [1:47:02<57:16,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19860/30456 [1:47:07<57:09,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19880/30456 [1:47:12<57:02,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19900/30456 [1:47:17<56:54,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19920/30456 [1:47:22<56:47,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  65% 19940/30456 [1:47:26<56:40,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 19960/30456 [1:47:31<56:32,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 19980/30456 [1:47:36<56:25,  3.09it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20000/30456 [1:47:41<56:18,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20020/30456 [1:47:46<56:10,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20040/30456 [1:47:51<56:03,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20060/30456 [1:47:55<55:56,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20080/30456 [1:48:00<55:48,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20100/30456 [1:48:05<55:41,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20120/30456 [1:48:10<55:34,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20140/30456 [1:48:15<55:26,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20160/30456 [1:48:19<55:19,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20180/30456 [1:48:24<55:12,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20200/30456 [1:48:29<55:05,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20220/30456 [1:48:34<54:57,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  66% 20240/30456 [1:48:39<54:50,  3.10it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20260/30456 [1:48:44<54:43,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20280/30456 [1:48:48<54:36,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20300/30456 [1:48:53<54:28,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20320/30456 [1:48:58<54:21,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20340/30456 [1:49:03<54:14,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20360/30456 [1:49:08<54:07,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20380/30456 [1:49:12<53:59,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20400/30456 [1:49:17<53:52,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20420/30456 [1:49:22<53:45,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20440/30456 [1:49:27<53:38,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20460/30456 [1:49:32<53:30,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20480/30456 [1:49:37<53:23,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20500/30456 [1:49:41<53:16,  3.11it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20520/30456 [1:49:46<53:09,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  67% 20540/30456 [1:49:51<53:02,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20560/30456 [1:49:56<52:54,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20580/30456 [1:50:01<52:47,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20600/30456 [1:50:05<52:40,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20620/30456 [1:50:10<52:33,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20640/30456 [1:50:15<52:26,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20660/30456 [1:50:20<52:19,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20680/30456 [1:50:25<52:11,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20700/30456 [1:50:30<52:04,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20720/30456 [1:50:34<51:57,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20740/30456 [1:50:39<51:50,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20760/30456 [1:50:44<51:43,  3.12it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20780/30456 [1:50:49<51:36,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20800/30456 [1:50:54<51:29,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20820/30456 [1:50:58<51:21,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20840/30456 [1:51:03<51:14,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  68% 20860/30456 [1:51:08<51:07,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 20880/30456 [1:51:13<51:00,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 20900/30456 [1:51:18<50:53,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 20920/30456 [1:51:23<50:46,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 20940/30456 [1:51:27<50:39,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 20960/30456 [1:51:32<50:32,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 20980/30456 [1:51:37<50:25,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21000/30456 [1:51:42<50:17,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21020/30456 [1:51:47<50:10,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21040/30456 [1:51:51<50:03,  3.13it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21060/30456 [1:51:56<49:56,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21080/30456 [1:52:01<49:49,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21100/30456 [1:52:06<49:42,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21120/30456 [1:52:11<49:35,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21140/30456 [1:52:16<49:28,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  69% 21160/30456 [1:52:20<49:21,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21180/30456 [1:52:25<49:14,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21200/30456 [1:52:30<49:07,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21220/30456 [1:52:35<49:00,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21240/30456 [1:52:40<48:53,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21260/30456 [1:52:44<48:46,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21280/30456 [1:52:49<48:39,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21300/30456 [1:52:54<48:32,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21320/30456 [1:52:59<48:25,  3.14it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21340/30456 [1:53:04<48:18,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21360/30456 [1:53:09<48:11,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21380/30456 [1:53:13<48:04,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21400/30456 [1:53:18<47:57,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21420/30456 [1:53:23<47:50,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21440/30456 [1:53:28<47:43,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  70% 21460/30456 [1:53:33<47:36,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21480/30456 [1:53:37<47:29,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21500/30456 [1:53:42<47:22,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21520/30456 [1:53:47<47:15,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21540/30456 [1:53:52<47:08,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21560/30456 [1:53:57<47:01,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21580/30456 [1:54:02<46:54,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21600/30456 [1:54:06<46:47,  3.15it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21620/30456 [1:54:11<46:40,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21640/30456 [1:54:16<46:33,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21660/30456 [1:54:21<46:26,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21680/30456 [1:54:26<46:19,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21700/30456 [1:54:30<46:12,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21720/30456 [1:54:35<46:05,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21740/30456 [1:54:40<45:58,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  71% 21760/30456 [1:54:45<45:51,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21780/30456 [1:54:50<45:44,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21800/30456 [1:54:55<45:37,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21820/30456 [1:54:59<45:30,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21840/30456 [1:55:04<45:23,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21860/30456 [1:55:09<45:17,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21880/30456 [1:55:14<45:10,  3.16it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21900/30456 [1:55:19<45:03,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21920/30456 [1:55:23<44:56,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21940/30456 [1:55:28<44:49,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21960/30456 [1:55:33<44:42,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 21980/30456 [1:55:38<44:35,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 22000/30456 [1:55:43<44:28,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 22020/30456 [1:55:48<44:21,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 22040/30456 [1:55:52<44:14,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 22060/30456 [1:55:57<44:08,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  72% 22080/30456 [1:56:02<44:01,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22100/30456 [1:56:07<43:54,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22120/30456 [1:56:12<43:47,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22140/30456 [1:56:16<43:40,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22160/30456 [1:56:21<43:33,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22180/30456 [1:56:26<43:26,  3.17it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22200/30456 [1:56:31<43:20,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22220/30456 [1:56:36<43:13,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22240/30456 [1:56:41<43:06,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22260/30456 [1:56:45<42:59,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22280/30456 [1:56:50<42:52,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22300/30456 [1:56:55<42:45,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22320/30456 [1:57:00<42:39,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22340/30456 [1:57:05<42:32,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22360/30456 [1:57:09<42:25,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  73% 22380/30456 [1:57:14<42:18,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22400/30456 [1:57:19<42:11,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22420/30456 [1:57:24<42:04,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22440/30456 [1:57:29<41:58,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22460/30456 [1:57:34<41:51,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22480/30456 [1:57:38<41:44,  3.18it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22500/30456 [1:57:43<41:37,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22520/30456 [1:57:48<41:30,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22540/30456 [1:57:53<41:24,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22560/30456 [1:57:58<41:17,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22580/30456 [1:58:02<41:10,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22600/30456 [1:58:07<41:03,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22620/30456 [1:58:12<40:56,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22640/30456 [1:58:17<40:50,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22660/30456 [1:58:22<40:43,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  74% 22680/30456 [1:58:27<40:36,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22700/30456 [1:58:31<40:29,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22720/30456 [1:58:36<40:23,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22740/30456 [1:58:41<40:16,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22760/30456 [1:58:46<40:09,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22780/30456 [1:58:51<40:02,  3.19it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22800/30456 [1:58:55<39:56,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22820/30456 [1:59:00<39:49,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22840/30456 [1:59:05<39:42,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22860/30456 [1:59:10<39:35,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22880/30456 [1:59:15<39:29,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22900/30456 [1:59:20<39:22,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22920/30456 [1:59:24<39:15,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22940/30456 [1:59:29<39:09,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22960/30456 [1:59:34<39:02,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  75% 22980/30456 [1:59:39<38:55,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23000/30456 [1:59:44<38:48,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23020/30456 [1:59:48<38:42,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23040/30456 [1:59:53<38:35,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23060/30456 [1:59:58<38:28,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23080/30456 [2:00:03<38:22,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23100/30456 [2:00:08<38:15,  3.20it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23120/30456 [2:00:13<38:08,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23140/30456 [2:00:17<38:02,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23160/30456 [2:00:22<37:55,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23180/30456 [2:00:27<37:48,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23200/30456 [2:00:32<37:41,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23220/30456 [2:00:37<37:35,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23240/30456 [2:00:41<37:28,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23260/30456 [2:00:46<37:21,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  76% 23280/30456 [2:00:51<37:15,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23300/30456 [2:00:56<37:08,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23320/30456 [2:01:01<37:01,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23340/30456 [2:01:05<36:55,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23360/30456 [2:01:10<36:48,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23380/30456 [2:01:15<36:41,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23400/30456 [2:01:20<36:35,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23420/30456 [2:01:25<36:28,  3.21it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23440/30456 [2:01:30<36:22,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23460/30456 [2:01:34<36:15,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23480/30456 [2:01:39<36:08,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23500/30456 [2:01:44<36:02,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23520/30456 [2:01:49<35:55,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23540/30456 [2:01:54<35:48,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23560/30456 [2:01:58<35:42,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23580/30456 [2:02:03<35:35,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  77% 23600/30456 [2:02:08<35:29,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23620/30456 [2:02:13<35:22,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23640/30456 [2:02:18<35:15,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23660/30456 [2:02:23<35:09,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23680/30456 [2:02:27<35:02,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23700/30456 [2:02:32<34:55,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23720/30456 [2:02:37<34:49,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23740/30456 [2:02:42<34:42,  3.22it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23760/30456 [2:02:47<34:36,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23780/30456 [2:02:51<34:29,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23800/30456 [2:02:56<34:23,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23820/30456 [2:03:01<34:16,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23840/30456 [2:03:06<34:09,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23860/30456 [2:03:11<34:03,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23880/30456 [2:03:16<33:56,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  78% 23900/30456 [2:03:20<33:50,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 23920/30456 [2:03:25<33:43,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 23940/30456 [2:03:30<33:36,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 23960/30456 [2:03:35<33:30,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 23980/30456 [2:03:40<33:23,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24000/30456 [2:03:44<33:17,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24020/30456 [2:03:49<33:10,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24040/30456 [2:03:54<33:04,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24060/30456 [2:03:59<32:57,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24080/30456 [2:04:04<32:51,  3.23it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24100/30456 [2:04:09<32:44,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24120/30456 [2:04:13<32:38,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24140/30456 [2:04:18<32:31,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24160/30456 [2:04:23<32:24,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24180/30456 [2:04:28<32:18,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  79% 24200/30456 [2:04:33<32:11,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24220/30456 [2:04:37<32:05,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24240/30456 [2:04:42<31:58,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24260/30456 [2:04:47<31:52,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24280/30456 [2:04:52<31:45,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24300/30456 [2:04:57<31:39,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24320/30456 [2:05:02<31:32,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24340/30456 [2:05:06<31:26,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24360/30456 [2:05:11<31:19,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24380/30456 [2:05:16<31:13,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24400/30456 [2:05:21<31:06,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24420/30456 [2:05:26<31:00,  3.24it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24440/30456 [2:05:30<30:53,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24460/30456 [2:05:35<30:47,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24480/30456 [2:05:40<30:40,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  80% 24500/30456 [2:05:45<30:34,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24520/30456 [2:05:50<30:27,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24540/30456 [2:05:55<30:21,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24560/30456 [2:05:59<30:14,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24580/30456 [2:06:04<30:08,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24600/30456 [2:06:09<30:01,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24620/30456 [2:06:14<29:55,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24640/30456 [2:06:19<29:48,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24660/30456 [2:06:23<29:42,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24680/30456 [2:06:28<29:36,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24700/30456 [2:06:33<29:29,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24720/30456 [2:06:38<29:23,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24740/30456 [2:06:43<29:16,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24760/30456 [2:06:48<29:10,  3.25it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24780/30456 [2:06:52<29:03,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24800/30456 [2:06:57<28:57,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  81% 24820/30456 [2:07:02<28:50,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24840/30456 [2:07:07<28:44,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24860/30456 [2:07:12<28:37,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24880/30456 [2:07:16<28:31,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24900/30456 [2:07:21<28:25,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24920/30456 [2:07:26<28:18,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24940/30456 [2:07:31<28:12,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24960/30456 [2:07:36<28:05,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 24980/30456 [2:07:41<27:59,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25000/30456 [2:07:45<27:52,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25020/30456 [2:07:50<27:46,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25040/30456 [2:07:55<27:40,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25060/30456 [2:08:00<27:33,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25080/30456 [2:08:05<27:27,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25100/30456 [2:08:09<27:20,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  82% 25120/30456 [2:08:14<27:14,  3.26it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25140/30456 [2:08:19<27:08,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25160/30456 [2:08:24<27:01,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25180/30456 [2:08:29<26:55,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25200/30456 [2:08:34<26:48,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25220/30456 [2:08:38<26:42,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25240/30456 [2:08:43<26:36,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25260/30456 [2:08:48<26:29,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25280/30456 [2:08:53<26:23,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25300/30456 [2:08:58<26:16,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25320/30456 [2:09:02<26:10,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25340/30456 [2:09:07<26:04,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25360/30456 [2:09:12<25:57,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25380/30456 [2:09:17<25:51,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25400/30456 [2:09:22<25:45,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  83% 25420/30456 [2:09:27<25:38,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25440/30456 [2:09:31<25:32,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25460/30456 [2:09:36<25:26,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25480/30456 [2:09:41<25:19,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25500/30456 [2:09:46<25:13,  3.27it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25520/30456 [2:09:51<25:06,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25540/30456 [2:09:55<25:00,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25560/30456 [2:10:00<24:54,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25580/30456 [2:10:05<24:47,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25600/30456 [2:10:10<24:41,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25620/30456 [2:10:15<24:35,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25640/30456 [2:10:20<24:28,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25660/30456 [2:10:24<24:22,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25680/30456 [2:10:29<24:16,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25700/30456 [2:10:34<24:09,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  84% 25720/30456 [2:10:39<24:03,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25740/30456 [2:10:44<23:57,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25760/30456 [2:10:48<23:50,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25780/30456 [2:10:53<23:44,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25800/30456 [2:10:58<23:38,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25820/30456 [2:11:03<23:31,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25840/30456 [2:11:08<23:25,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25860/30456 [2:11:13<23:19,  3.28it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25880/30456 [2:11:17<23:12,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25900/30456 [2:11:22<23:06,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25920/30456 [2:11:27<23:00,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25940/30456 [2:11:32<22:53,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25960/30456 [2:11:37<22:47,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 25980/30456 [2:11:41<22:41,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 26000/30456 [2:11:46<22:35,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  85% 26020/30456 [2:11:51<22:28,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26040/30456 [2:11:56<22:22,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26060/30456 [2:12:01<22:16,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26080/30456 [2:12:06<22:09,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26100/30456 [2:12:10<22:03,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26120/30456 [2:12:15<21:57,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26140/30456 [2:12:20<21:51,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26160/30456 [2:12:25<21:44,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26180/30456 [2:12:30<21:38,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26200/30456 [2:12:34<21:32,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26220/30456 [2:12:39<21:25,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26240/30456 [2:12:44<21:19,  3.29it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26260/30456 [2:12:49<21:13,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26280/30456 [2:12:54<21:07,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26300/30456 [2:12:59<21:00,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26320/30456 [2:13:03<20:54,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  86% 26340/30456 [2:13:08<20:48,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26360/30456 [2:13:13<20:42,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26380/30456 [2:13:18<20:35,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26400/30456 [2:13:23<20:29,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26420/30456 [2:13:27<20:23,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26440/30456 [2:13:32<20:17,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26460/30456 [2:13:37<20:10,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26480/30456 [2:13:42<20:04,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26500/30456 [2:13:47<19:58,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26520/30456 [2:13:51<19:52,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26540/30456 [2:13:56<19:45,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26560/30456 [2:14:01<19:39,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26580/30456 [2:14:06<19:33,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26600/30456 [2:14:11<19:27,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26620/30456 [2:14:16<19:20,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  87% 26640/30456 [2:14:20<19:14,  3.30it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26660/30456 [2:14:25<19:08,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26680/30456 [2:14:30<19:02,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26700/30456 [2:14:35<18:55,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26720/30456 [2:14:40<18:49,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26740/30456 [2:14:44<18:43,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26760/30456 [2:14:49<18:37,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26780/30456 [2:14:54<18:31,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26800/30456 [2:14:59<18:24,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26820/30456 [2:15:04<18:18,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26840/30456 [2:15:09<18:12,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26860/30456 [2:15:13<18:06,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26880/30456 [2:15:18<18:00,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26900/30456 [2:15:23<17:53,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26920/30456 [2:15:28<17:47,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  88% 26940/30456 [2:15:33<17:41,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 26960/30456 [2:15:37<17:35,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 26980/30456 [2:15:42<17:29,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27000/30456 [2:15:47<17:22,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27020/30456 [2:15:52<17:16,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27040/30456 [2:15:57<17:10,  3.31it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27060/30456 [2:16:02<17:04,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27080/30456 [2:16:06<16:58,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27100/30456 [2:16:11<16:51,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27120/30456 [2:16:16<16:45,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27140/30456 [2:16:21<16:39,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27160/30456 [2:16:26<16:33,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27180/30456 [2:16:30<16:27,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27200/30456 [2:16:35<16:21,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27220/30456 [2:16:40<16:14,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  89% 27240/30456 [2:16:45<16:08,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27260/30456 [2:16:50<16:02,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27280/30456 [2:16:55<15:56,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27300/30456 [2:16:59<15:50,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27320/30456 [2:17:04<15:44,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27340/30456 [2:17:09<15:37,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27360/30456 [2:17:14<15:31,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27380/30456 [2:17:19<15:25,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27400/30456 [2:17:23<15:19,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27420/30456 [2:17:28<15:13,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27440/30456 [2:17:33<15:07,  3.32it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27460/30456 [2:17:38<15:01,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27480/30456 [2:17:43<14:54,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27500/30456 [2:17:48<14:48,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27520/30456 [2:17:52<14:42,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27540/30456 [2:17:57<14:36,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  90% 27560/30456 [2:18:02<14:30,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27580/30456 [2:18:07<14:24,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27600/30456 [2:18:12<14:18,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27620/30456 [2:18:16<14:11,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27640/30456 [2:18:21<14:05,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27660/30456 [2:18:26<13:59,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27680/30456 [2:18:31<13:53,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27700/30456 [2:18:36<13:47,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27720/30456 [2:18:41<13:41,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27740/30456 [2:18:45<13:35,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27760/30456 [2:18:50<13:29,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27780/30456 [2:18:55<13:22,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27800/30456 [2:19:00<13:16,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27820/30456 [2:19:05<13:10,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27840/30456 [2:19:09<13:04,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  91% 27860/30456 [2:19:14<12:58,  3.33it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 27880/30456 [2:19:19<12:52,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 27900/30456 [2:19:24<12:46,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 27920/30456 [2:19:29<12:40,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 27940/30456 [2:19:34<12:34,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 27960/30456 [2:19:38<12:27,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 27980/30456 [2:19:43<12:21,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28000/30456 [2:19:48<12:15,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28020/30456 [2:19:53<12:09,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28040/30456 [2:19:58<12:03,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28060/30456 [2:20:02<11:57,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28080/30456 [2:20:07<11:51,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28100/30456 [2:20:12<11:45,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28120/30456 [2:20:17<11:39,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28140/30456 [2:20:22<11:33,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  92% 28160/30456 [2:20:27<11:27,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28180/30456 [2:20:31<11:21,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28200/30456 [2:20:36<11:14,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28220/30456 [2:20:41<11:08,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28240/30456 [2:20:46<11:02,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28260/30456 [2:20:51<10:56,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28280/30456 [2:20:55<10:50,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28300/30456 [2:21:00<10:44,  3.34it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28320/30456 [2:21:05<10:38,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28340/30456 [2:21:10<10:32,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28360/30456 [2:21:15<10:26,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28380/30456 [2:21:20<10:20,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28400/30456 [2:21:24<10:14,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28420/30456 [2:21:29<10:08,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28440/30456 [2:21:34<10:02,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  93% 28460/30456 [2:21:39<09:56,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28480/30456 [2:21:44<09:50,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28500/30456 [2:21:48<09:43,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28520/30456 [2:21:53<09:37,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28540/30456 [2:21:58<09:31,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28560/30456 [2:22:03<09:25,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28580/30456 [2:22:08<09:19,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28600/30456 [2:22:13<09:13,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28620/30456 [2:22:17<09:07,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28640/30456 [2:22:22<09:01,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28660/30456 [2:22:27<08:55,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28680/30456 [2:22:32<08:49,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28700/30456 [2:22:37<08:43,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28720/30456 [2:22:41<08:37,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28740/30456 [2:22:46<08:31,  3.35it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28760/30456 [2:22:51<08:25,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  94% 28780/30456 [2:22:56<08:19,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28800/30456 [2:23:01<08:13,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28820/30456 [2:23:06<08:07,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28840/30456 [2:23:10<08:01,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28860/30456 [2:23:15<07:55,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28880/30456 [2:23:20<07:49,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28900/30456 [2:23:25<07:43,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28920/30456 [2:23:30<07:37,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28940/30456 [2:23:34<07:31,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28960/30456 [2:23:39<07:25,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 28980/30456 [2:23:44<07:19,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 29000/30456 [2:23:49<07:13,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 29020/30456 [2:23:54<07:07,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 29040/30456 [2:23:59<07:01,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 29060/30456 [2:24:03<06:55,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  95% 29080/30456 [2:24:08<06:49,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29100/30456 [2:24:13<06:43,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29120/30456 [2:24:18<06:37,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29140/30456 [2:24:23<06:31,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29160/30456 [2:24:27<06:25,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29180/30456 [2:24:32<06:19,  3.36it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29200/30456 [2:24:37<06:13,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29220/30456 [2:24:42<06:07,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29240/30456 [2:24:47<06:01,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29260/30456 [2:24:52<05:55,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29280/30456 [2:24:56<05:49,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29300/30456 [2:25:01<05:43,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29320/30456 [2:25:06<05:37,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29340/30456 [2:25:11<05:31,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29360/30456 [2:25:16<05:25,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  96% 29380/30456 [2:25:20<05:19,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29400/30456 [2:25:25<05:13,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29420/30456 [2:25:30<05:07,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29440/30456 [2:25:35<05:01,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29460/30456 [2:25:40<04:55,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29480/30456 [2:25:44<04:49,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29500/30456 [2:25:49<04:43,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29520/30456 [2:25:54<04:37,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29540/30456 [2:25:59<04:31,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29560/30456 [2:26:04<04:25,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29580/30456 [2:26:09<04:19,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29600/30456 [2:26:13<04:13,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29620/30456 [2:26:18<04:07,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29640/30456 [2:26:23<04:01,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29660/30456 [2:26:28<03:55,  3.37it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  97% 29680/30456 [2:26:33<03:49,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29700/30456 [2:26:37<03:43,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29720/30456 [2:26:42<03:37,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29740/30456 [2:26:47<03:32,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29760/30456 [2:26:52<03:26,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29780/30456 [2:26:57<03:20,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29800/30456 [2:27:02<03:14,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29820/30456 [2:27:06<03:08,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29840/30456 [2:27:11<03:02,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29860/30456 [2:27:16<02:56,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29880/30456 [2:27:21<02:50,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29900/30456 [2:27:26<02:44,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29920/30456 [2:27:30<02:38,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29940/30456 [2:27:35<02:32,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29960/30456 [2:27:40<02:26,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  98% 29980/30456 [2:27:45<02:20,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30000/30456 [2:27:50<02:14,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30020/30456 [2:27:55<02:08,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30040/30456 [2:27:59<02:02,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30060/30456 [2:28:04<01:57,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30080/30456 [2:28:09<01:51,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30100/30456 [2:28:14<01:45,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30120/30456 [2:28:19<01:39,  3.38it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30140/30456 [2:28:23<01:33,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30160/30456 [2:28:28<01:27,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30180/30456 [2:28:33<01:21,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30200/30456 [2:28:38<01:15,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30220/30456 [2:28:43<01:09,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30240/30456 [2:28:48<01:03,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30260/30456 [2:28:52<00:57,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30280/30456 [2:28:57<00:51,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0:  99% 30300/30456 [2:29:02<00:46,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30320/30456 [2:29:07<00:40,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30340/30456 [2:29:12<00:34,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30360/30456 [2:29:16<00:28,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30380/30456 [2:29:21<00:22,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30400/30456 [2:29:26<00:16,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30420/30456 [2:29:31<00:10,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Epoch 0: 100% 30440/30456 [2:29:36<00:04,  3.39it/s, loss=1.43, v_num=23, val_loss=9.220, train_loss=1.280]\n",
            "Validating: 100% 27100/27109 [1:48:47<00:02,  4.15it/s]\u001b[A\n",
            "Epoch 0: 100% 30456/30456 [2:29:43<00:00,  3.39it/s, loss=1.46, v_num=23, val_loss=1.420, train_loss=1.340]\n",
            "                                                       \u001b[AEpoch 0, global step 3346: val_loss reached 1.41611 (best 1.41611), saving model to \"/content/drive/MyDrive/Colab_Notebooks/NLP/BART/kobart_translation-model_chp/epoch=00-val_loss=1.416.ckpt\" as top 1\n",
            "INFO:lightning:Epoch 0, global step 3346: val_loss reached 1.41611 (best 1.41611), saving model to \"/content/drive/MyDrive/Colab_Notebooks/NLP/BART/kobart_translation-model_chp/epoch=00-val_loss=1.416.ckpt\" as top 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "tcmalloc: large alloc 1362485248 bytes == 0xfbcf6000 @  0x7f35477b0615 0x592b76 0x4df71e 0x59394f 0x593bc6 0x7f352f683a94 0x7f352f685864 0x7f352f655590 0x7f351ff9b465 0x7f351ff979ca 0x7f351ff9c609 0x7f352f658f2b 0x7f352f2de200 0x593784 0x594731 0x548cc1 0x51566f 0x549e0e 0x593fce 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x511e2c 0x549576 0x593fce\n",
            "tcmalloc: large alloc 1703108608 bytes == 0x14d054000 @  0x7f35477b0615 0x592b76 0x4df71e 0x59394f 0x593bc6 0x7f352f683a94 0x7f352f685864 0x7f352f655590 0x7f351ff9b465 0x7f351ff979ca 0x7f351ff9c609 0x7f352f658f2b 0x7f352f2de200 0x593784 0x594731 0x548cc1 0x51566f 0x549e0e 0x593fce 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x511e2c 0x549576 0x593fce\n",
            "Epoch 0: 100% 30456/30456 [2:29:49<00:00,  3.39it/s, loss=1.46, v_num=23, val_loss=1.420, train_loss=1.340]tcmalloc: large alloc 1703108608 bytes == 0xbad78000 @  0x7f35477b0615 0x592b76 0x4df71e 0x59394f 0x593bc6 0x7f352f683a94 0x7f352f685864 0x7f352f655590 0x7f351ff9b465 0x7f351ff979ca 0x7f351ff9c609 0x7f352f658f2b 0x7f352f2de200 0x593784 0x594731 0x548cc1 0x51566f 0x549e0e 0x593fce 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x511e2c 0x549576 0x593fce\n",
            "Saving latest checkpoint...\n",
            "INFO:lightning:Saving latest checkpoint...\n",
            "Epoch 0: 100% 30456/30456 [2:30:30<00:00,  3.37it/s, loss=1.46, v_num=23, val_loss=1.420, train_loss=1.340]\n"
          ]
        }
      ],
      "source": [
        "!python train.py  --gradient_clip_val 1.0 --max_epochs 1 --default_root_dir /content/drive/MyDrive/Colab_Notebooks/NLP/BART --gpus 1 \\\n",
        "--batch_size 9 --num_workers 1 --checkpoint_path /content/drive/MyDrive/Colab_Notebooks/NLP/BART \\\n",
        "--train_file /content/test.tsv \\\n",
        "--test_file /content/train.tsv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_model_binary.py --hparams /content/drive/MyDrive/Colab_Notebooks/NLP/BART/tb_logs/default/version_23/hparams.yaml \\\n",
        "                            --model_binary /content/drive/MyDrive/Colab_Notebooks/NLP/BART/kobart_translation-model_chp/epoch=00-val_loss=1.416.ckpt"
      ],
      "metadata": {
        "id": "nXTmYmRL8495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b910bd75-4b8e-40c6-dd18-b7d41290127a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"get_model_binary.py\", line 2, in <module>\n",
            "    from train import KoBARTConditionalGeneration\n",
            "  File \"/content/kobart_summarization/train.py\", line 6, in <module>\n",
            "    import pytorch_lightning as pl\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/__init__.py\", line 62, in <module>\n",
            "    from pytorch_lightning import metrics\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/__init__.py\", line 14, in <module>\n",
            "    from pytorch_lightning.metrics.classification import (  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/__init__.py\", line 14, in <module>\n",
            "    from pytorch_lightning.metrics.classification.accuracy import Accuracy  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/accuracy.py\", line 18, in <module>\n",
            "    from pytorch_lightning.metrics.functional.accuracy import _accuracy_compute, _accuracy_update\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/functional/__init__.py\", line 14, in <module>\n",
            "    from pytorch_lightning.metrics.functional.accuracy import accuracy  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/functional/accuracy.py\", line 18, in <module>\n",
            "    from pytorch_lightning.metrics.classification.helpers import _input_format_classification, DataType\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/helpers.py\", line 19, in <module>\n",
            "    from pytorch_lightning.metrics.utils import select_topk, to_onehot\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/utils.py\", line 18, in <module>\n",
            "    from pytorch_lightning.utilities import rank_zero_warn\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/__init__.py\", line 18, in <module>\n",
            "    from pytorch_lightning.utilities.apply_func import move_data_to_device  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/apply_func.py\", line 28, in <module>\n",
            "    from torchtext.data import Batch\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/__init__.py\", line 40, in <module>\n",
            "    _init_extension()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/__init__.py\", line 36, in _init_extension\n",
            "    torch.ops.load_library(ext_specs.origin)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_ops.py\", line 220, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.7/ctypes/__init__.py\", line 364, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: /usr/local/lib/python3.7/dist-packages/torchtext/_torchtext.so: undefined symbol: _ZNK3c104Type14isSubtypeOfExtESt10shared_ptrIS0_EPSo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZrD9QR6nWe7"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltaXcrzAZdtz",
        "outputId": "6c176617-8aa1-4183-9c42-09fc35fb03b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0xTiOETfgbk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "tf = open('test.tsv','w',encoding='utf-8',newline='') # tsv파일 하나 생성\n",
        "outfile = csv.writer(tf, delimiter='\\t') # 그 파일을 구문자 \\t로 읽음\n",
        "outfile.writerow(['original','summary']) # original, summary로 구분(원문, 요약문)\n",
        "\n",
        "count = 0\n",
        "# 데이터 전처리\n",
        "file = '/content/drive/Shareddrives/CS2022/valid_original.json'\n",
        "\n",
        "with open(file, 'r', encoding='UTF-8-sig') as f:\n",
        "  json_data = json.load(f)\n",
        "  for document_list in json_data['documents']:  \n",
        "    # count += 1\n",
        "    original, summarized = '', document_list['abstractive'][0] # 원문, 요약문\n",
        "    for main_text in document_list['text']:\n",
        "      for sentence in main_text:\n",
        "        original += sentence['sentence']  # text리스트 중 sentence요소만 빼서 original에 추가\n",
        "    if original != \"\" and summarized != \"\":\n",
        "      outfile.writerow([original,summarized])  # 원문과 요약문 집어넣기\n",
        "    # if count == 100000: \n",
        "    #   break\n",
        "\n",
        "tf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDM8oXp7DhGj",
        "outputId": "5acc8ed0-989f-43cd-99c7-55b72cab3d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.4.1\n",
            "  Using cached PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytorch-lightning 1.2.2 requires PyYAML!=5.4.*,>=5.1, but you have pyyaml 5.4.1 which is incompatible.\n",
            "kobart 0.5.1 requires pytorch-lightning==1.2.1, but you have pytorch-lightning 1.2.2 which is incompatible.\n",
            "kobart 0.5.1 requires torch==1.7.1, but you have torch 1.11.0+rocm4.5.2 which is incompatible.\n",
            "kobart 0.5.1 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\n",
            "awscli 1.24.6 requires docutils<0.17,>=0.10, but you have docutils 0.17.1 which is incompatible.\n",
            "awscli 1.24.6 requires rsa<4.8,>=3.1.2, but you have rsa 4.8 which is incompatible.\u001b[0m\n",
            "Successfully installed pyyaml-5.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml==5.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUfNe5MdnaU7"
      },
      "source": [
        "시각화"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHGxydh7cDEU",
        "outputId": "42ae43b6-fcb1-4e2f-f3a1-54cb15d4957b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (5.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwCczttGc4xS",
        "outputId": "a5e3a5ed-d45d-4ee3-893a-5e426ede975a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 28gaTNsqsblrWmvyZrvI9kRsToH_4hnBZGELbZxWJaBVWcDnw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlsBwKGSdYkW"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyHciKnsdczE"
      },
      "outputs": [],
      "source": [
        "!streamlit run infer.py&>dev>null&"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS-tKW1ndjmZ"
      },
      "outputs": [],
      "source": [
        "publ_url = ngrok.connect(addr='8501')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYH3Z7S6djsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90bfb289-7cf7-4aad-d86b-3f1b87472d85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://094c-35-197-119-83.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "publ_url"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}